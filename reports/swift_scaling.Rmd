SWIFT model simulation scaling measures
=================

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(out.extra='style="display:block; margin: auto"', fig.align="center", fig.width=10, fig.height=8, dev='png')
```

```{r, eval=FALSE, include=FALSE}
library(swiftdev)
thisPkgDir <- file.path('f:/src/github_jm/hwrs2015')
if(file.exists(thisPkgDir))
{
  setwd(file.path(thisPkgDir, 'reports'))
  purlAll('.','.')
}
```

# About this document

This document was generated from an R markdown file on `r as.character(Sys.time())`. The report is generated using:

```{r, eval=FALSE, include=TRUE}
library(knitr)
setwd('F:/src/github_jm/hwrs2015/reports')
knit2html('swift_scaling.Rmd')
```
# Methods

The dimensions of the problem for which we want to assess how SWIFT scales are:
* system size
* number of time steps
* level of parallel runs (multi-threading)

A couple of other dimensions are of interest, but not elaborated on yet.
* number of input time series
* number of output time series

You may want to only skim through the method section and go straight to the observations/results section at the end.

```{r}
library(ophct)
library(calibragem)# just to use memory cleanup features.
# library(microbenchmark) # not yet needed; may be later 
# library(DiagrammeR)
library(dplyr)
```
# Methods - details

## Test data

First we use some sample data from the package, from the Ovens catchment. We extend the time series to a maximum of 60 years by recycling the data of a decade

```{r}
simulStart <- '2000-01-01'
simulEnd <- '2009-12-31'
sSpan <- paste0(simulStart, '/', simulEnd)

rain <- sampleSeries('Ovens', 'rain')[sSpan]
evap <- sampleSeries('Ovens', 'evap')[sSpan]
flow <- sampleSeries('Ovens', 'flow')[sSpan]

head(index(rain))

extendTs <- function(tSeries) {
  d <- as.vector(tSeries)
  d <- rep(d, 7)
  res <- xts(d, order.by=(index(tSeries)[1] + 3600*(0:(length(d)-1))))
  return(res['2000-01-01/2059-12-31'])
}

rain <- extendTs(rain)
evap <- extendTs(evap)
flow <- extendTs(flow)
```

## Functions

This section declares the functions used to set up the models being benchmark. You do not need to read through it, and can skip to the section Observations

We measure the runtime of catchment node-link networks idealised as a tree structure (structurally close to reality)

```{r}
numLinks <- function(streamOrder=1) {
  # keeps numLinks vectorizable
  if(length(streamOrder)==1) {
	return(sum(2**(1:streamOrder)))
  } else {
	return(sapply( streamOrder, function(n) {sum(2**(1:n))} ))
  }
}

defineNetwork <- function(streamOrder=1, runoffModel='AWBM') {
# o    o  o    o 
#  \  /    \  /    stream order 2
#    o      o
#     \    /      stream order 1
#       o
  # numLinks(0) = 0
  # numLinks(1) = 2
  # numLinks(2) = 4
  # numLinks(n) = numLinks(n-1)*2
  # so:
  # numLinks = sum(1->n)(2**n)

  # numNodes(0) = 1
  # numNodes(1) = 2
  # numNodes(2) = 4
  # numNodes(n) = numNodes(n-1)*2
  # numNodes = sum(1->n)(2**n) + 1

  nl <- numLinks(streamOrder)
  linkIds <- paste0("link_id_", 1:nl)
  nodeIds <- paste0("node_id_", 1:(nl+1))

  defn <- list(
    nodeIds=nodeIds,
    nodeNames = paste0(nodeIds, '_name'),
    linkIds=linkIds,
    linkNames = paste0(linkIds, '_name'),
    fromNode = nodeIds[2:length(nodeIds)], # 2,3,4,5,6,7,8,...
    toNode = rep(nodeIds, each=2)[1:(length(nodeIds)-1)], # 1,1,2,2,3,3,4,4...
    areasKm2 = rep(1.0, nl),
    runoffModel = runoffModel
  )
  defn
}
```

Functions that build the test model structure, and set up the the simulation (inputs and simulation span)

```{r}
mkModelStructure <- function(defn) {
  createCatchment(
      defn$nodeIds, defn$nodeNames, 
      defn$linkIds, defn$linkNames, 
      defn$fromNode, defn$toNode, 
      defn$runoffModel, defn$areasKm2)
}
mkModel <- function(streamOrder, tstep='hourly') {
  simulation <- mkModelStructure(defineNetwork(streamOrder))
  setSimulationSpan(simulation, start(rain), end(rain))
  setSimulationTimeStep(simulation, tstep)
  simulation <- swapModel(simulation, 'MuskingumNonLinear', 'channel_routing')
  return(simulation)
}

mkTypical <- function(streamOrder, years=10, tstep='hourly') {
  sim <- mkModel(streamOrder) 
  sIds <- getSubareaIds(sim)
  simulStart <- '2000-01-01'
  simulEnd <- paste0(2000+years-1, '-12-31')
  sSpan <- paste0('2000-01-01/',2000+years-1, '-12-31')
  rain <- rain[sSpan]
  evap <- evap[sSpan]
  for (s in sIds) {
	# Note that input time series are always fully copied for each subarea, so 
	# this setup is faithfully mimicking a system with spatially varying inputs.
    playSubareaInput(sim, input=rain, s, 'Rainfall')
    playSubareaInput(sim, input=evap, s, 'Evapotranspiration')
  }
  setSimulationSpan(sim, start(rain), end(rain))
  setSimulationTimeStep(sim, tstep)
  recordState(sim, 'Catchment.StreamflowRate')
  return(sim)
}

```

Functions running the simulation. We use a C# binding layer to handle multi-threaded SWIFT test runs and nanosecond-level timing stopwatch. For our test case this would be cumbersome to do with R benchmarking packages.

```{r}
clrLoadAssembly('f:/src/github_jm/hwrs2015/ParallelSwift/bin/Debug/ParallelSwift.dll')

runParallel <- function(simulation, nthreads=1L, reps=1L) {
	return(clrCallStatic('ParallelSwift.SwiftRunTests', 'ParallelRun', simulation, nthreads, reps, -1L))
}
		
runCase <- function(streamOrder, years=10L, nthreads=1L, reps=1L) {
  s <- mkTypical(streamOrder, years)
  runParallel(s, nthreads=nthreads, reps=reps)
}

runAndTag <- function(case, reps=10L) {
  streamOrder <- as.integer(case$so)
  years <- as.integer(case$years)
  nthreads <- as.integer(case$threads)
  rt <- runCase(streamOrder=streamOrder, years=years, nthreads=nthreads, reps=reps)
  data.frame(deltaT=rt, streamOrder=rep(streamOrder, reps), nthreads=rep(nthreads, reps), years=rep(years, reps))
}

```

# Observations

We define a matrix of test cases, with factors being the stream order of the network, length of simulation in years, and number of systems to run in parallel, multi-threaded.

```{r}
cases <- expand.grid(so=1:5, years=c(2,5,10,20,30), threads=1:6)
#cases <- cases[nrow(cases):1,]

benchmarkRun <- function(i, reps=10L) { 
  calibragem::forceMemoryCollection() # reduce the risk of interference of blocking garbage collection - cleans the slates in both R and .NET GC, and consequently SWIFT C++.
  runAndTag(cases[i,], reps) 
}
```


```{r}

resultsFile <- 'f:/STSF/hwrs.RData'
if(file.exists(resultsFile)) {
  load(resultsFile)
} else {
  s <- now()
  measures <- lapply( 1:nrow(cases), benchmarkRun )
  e <- now()
  e-s
  measures <- rbind_all(measures)
}
measures
measures$nlinks <- numLinks(measures$streamOrder)
```

Define a column rt as the length runtime per simulation year and per sub-area in the sytstem. This is needed to compare "apple with apple". Note that the number of threads need not be included, as the method to assess multi-threading already returns a comparable measure.

```{r}
measures$rt = measures$deltaT / (measures$years * measures$nlinks)
```

Let's see if we can resonably use the median of each 10 replicate measure for each case:

```{r}
by_cat <- group_by(measures, nlinks, nthreads, years)

tmp <- summarise(by_cat, cv = sd(deltaT) / median(deltaT))
summary(tmp$cv)
```

Most coefficients of variations are less than a percent. Let's define a performance measure perf by standardizing against the case with the smallest 'rt'

```{r}
m <- summarise(by_cat, rt=median(rt), deltaT=median(deltaT))
m$perf = min(m$rt)/m$rt
m$years.simul = factor(m$years)

p <- ggplot(m, aes(x = nthreads, y = perf, colour=years.simul))
#p <- ggplot(m, aes(x = nthreads, y = perf))
p <- p + geom_point()
p <- p + facet_grid(. ~ nlinks, scales = "free")
#p <- p + facet_grid(years ~ nlinks, scales = "free")
p + labs (title = "Relative speed performance, facet by number of subcatchments", y="Speed performance" ,
   x = "number of threads", colour = "Simul. years")
```

This plot is rich in information.
* The system scales well with multiple threads, for a given model size and length of simulation.
* The system scales well with increasing system size. A drop in performance with system size is expected, typically because memory cache misses are more likely than with smaller models.

```{r}
sOrder <- 4
cases <- expand.grid(so=sOrder, years=c(1:9,10,15,20,25,30,35,40,45,50,55,60), threads=1)
if(!file.exists(resultsFile)) {
  s <- now()
  mSimlength <- lapply( 1:nrow(cases), benchmarkRun )
  e <- now()
  e-s
  mSimlength <- rbind_all(mSimlength)
  save(measures, mSimlength, file=resultsFile)
}
mSimlength$nlinks <- numLinks(mSimlength$streamOrder)


by_cat <- group_by(mSimlength, nlinks, nthreads, years)
tmp <- summarise(by_cat, cv = sd(deltaT) / median(deltaT))
summary(tmp$cv)

m <- summarise(by_cat, deltaT=median(deltaT))
m$rtPerYear = m$deltaT / m$years

p <- ggplot(m, aes(x = years, y=rtPerYear))
p <- p + geom_point()
p + labs (title = paste0("Runtime per simulation year, subareas= ", numLinks(sOrder)), y="Runtime per simulation year (ms)" ,
   x = "Number of simulation years")
```


```{r, eval=FALSE, include=FALSE}

nReps <- 10L


cpSimul <- function(s) {
  return(clrCall(s, 'CloneModel'))
}
getMeanTime <- function(mbench) {
  m <- as.data.frame(mbench)
  m_g <- group_by(m, expr)
  summarise(m_g, time = mean(time, na.rm = TRUE))
}


baseline <- mkModel(3) 

s1 <- cpSimul(baseline)
sIds <- getSubareaIds(s1)

for (s in sIds) {
  playSubareaInput(s1, input=rain, s, 'Rainfall')
}
s2 <- cpSimul(s1)
for (s in sIds) {
  playSubareaInput(s2, input=evap, s, 'Evapotranspiration')
}

s3 <- cpSimul(s2)
for (s in sIds) { recordState(s3, paste('subarea', s, 'Runoff', sep='.')) }

s4 <- cpSimul(s3)
for (s in sIds) { recordState(s4, paste('subarea', s, 'Baseflow', sep='.')) }

systems <- list(baseline, s1, s2, s3, s4)
f <- function(i) {execSimulation(systems[[i]])}

# mb <- microbenchmark(f(1), f(2), f(3), f(4), f(5), times=nReps, control = list(order='block'))
# plot(mb, ylab = 'time (ns)')


systems <- lapply( 1:4, function(i) { mkTypical(i) } )

mb <- microbenchmark(f(1),f(2),f(3),f(4), times=nReps, control = list(order='block'))


plot(mb, ylab = 'time (ns)')
m <- getMeanTime(mb)
plot( sqrt(time) ~ expr, data=m)

d <- getCatchmentDotGraph(s4)
DiagrammeR(d)
```
